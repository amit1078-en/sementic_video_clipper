{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic video cutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wave\n",
    "import json\n",
    "\n",
    "import moviepy.editor as mp\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "# to install with jupyter uncomment and run next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install moviepy vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom import\n",
    "import Word as custom_Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_audio_vosk(audio_path, model):\n",
    "    '''\n",
    "    Recognize audio using vosk model.\n",
    "    Language of the recognition depends on model.\n",
    "    Returns list of Word objects. Each of them has the following attributes:\n",
    "        conf (float): degree of confidence, from 0 to 1\n",
    "        end (float): end time of the pronouncing the word, in seconds\n",
    "        start (float): start time of the pronouncing the word, in seconds\n",
    "        word (str): recognized word\n",
    "\n",
    "    Parameters:\n",
    "        audio_path (str): path to the audio file to recognize. Must be WAV format mono PCM\n",
    "        model: vosk model. Must be loaded with `model = Model(model_path)` command\n",
    "\n",
    "    Returns:\n",
    "        list_of_Words (array): list of Word objects\n",
    "    '''\n",
    "    \n",
    "    # check if audio is mono wav\n",
    "    wf = wave.open(audio_path, \"rb\")\n",
    "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "        print(\"Audio file must be WAV format mono PCM\")\n",
    "        sys.exit()\n",
    "\n",
    "    rec = KaldiRecognizer(model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    print('\\n\\tStarting to convert audio to text. It may take some time...')\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = []\n",
    "    # recognize speech using vosk model\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            part_result = json.loads(rec.Result())\n",
    "            results.append(part_result)\n",
    "\n",
    "    part_result = json.loads(rec.FinalResult())\n",
    "    results.append(part_result)\n",
    "\n",
    "    # convert list of JSON dictionaries to list of 'Word' objects\n",
    "    list_of_Words = []\n",
    "    for sentence in results:\n",
    "        if len(sentence) == 1:\n",
    "            # sometimes there are bugs in recognition \n",
    "            # and it returns an empty dictionary\n",
    "            # {'text': ''}\n",
    "            continue\n",
    "        for obj in sentence['result']:\n",
    "            w = custom_Word.Word(obj)  # create custom Word object\n",
    "            list_of_Words.append(w)  # and add it to list\n",
    "\n",
    "    # forming a final string from the words\n",
    "    text = ''\n",
    "    for w in list_of_Words:\n",
    "        text += w.word + ' '\n",
    "\n",
    "    time_elapsed = time.strftime('%H:%M:%S',\n",
    "                                 time.gmtime(time.time() - start_time))\n",
    "    print(f'Done! Elapsed time = {time_elapsed}')\n",
    "\n",
    "    print(\"\\n\\tVosk thinks you said:\\n\")\n",
    "    print(text)\n",
    "    \n",
    "    wf.close  # close audiofile\n",
    "\n",
    "    return list_of_Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments_from_audio_control_words(list_of_Words, start_word='начало', end_word='конец', offset=0.5):\n",
    "    '''\n",
    "    Parse list of Word objects for 'start_word' and 'end_word' \n",
    "    and returns 'segments' - list of tuples, where each turple is\n",
    "    (start_time of start_word - offset, end_time of end_word + offset)\n",
    "\n",
    "    Parameters:\n",
    "        list_of_Words (array): list of Word objects. \n",
    "                               Received from `recognize_audio_vosk()` function\n",
    "        start_word (str): control word that signals the beginning of the video fragment to be cut\n",
    "        end_word (str): control word that signals the ending of the video fragment to be cut\n",
    "        offset (float): offset in seconds. Number being subtracted from 'start_time' for 'start_word' \n",
    "                        and added to 'end_time' for 'end_word'\n",
    "\n",
    "    Returns:\n",
    "        segments (array): list of tuples (start_time, end_time)\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\\tStarting the search for control words...\")\n",
    "\n",
    "    # lists for start and end times\n",
    "    starts = []\n",
    "    ends = []\n",
    "\n",
    "    # cycle by all Words\n",
    "    for w in list_of_Words:\n",
    "        if w.word == start_word:\n",
    "            starts.append(w.start - offset)\n",
    "        if w.word == end_word:\n",
    "            ends.append(w.end + offset)\n",
    "\n",
    "    # from starts and ends to segments\n",
    "    # starts = [1, 3], ends = [2, 4] ->\n",
    "    # segments = (0, 1), (2, 3), (4, None)\n",
    "\n",
    "    segments = []\n",
    "    length = max(len(starts), len(ends))\n",
    "    for i in range(length + 1):\n",
    "        if i == 0:\n",
    "            segments.append((0, starts[0]))\n",
    "        elif i == length:\n",
    "            segments.append((ends[i-1], None))\n",
    "        else:\n",
    "            # intermediate values\n",
    "            segments.append((ends[i-1], starts[i]))\n",
    "    print(\"The search of control words is completed. Got the following array of segments: \\n\")\n",
    "    print(segments)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments_from_audio_silence(list_of_Words, threshold=2, offset=1):\n",
    "    '''\n",
    "    Parse list of Word objects for silence.\n",
    "    If silence lasts longer than treshold value, this fragment will be cut.\n",
    "    Returns 'segments' - list of tuples, where each turple is\n",
    "    (start_time, end_time)\n",
    "\n",
    "    Parameters:\n",
    "        list_of_Words (array): list of Word objects. \n",
    "                               Received from `recognize_audio_vosk()` function\n",
    "        threshold (float): treshold value in seconds\n",
    "        offset (float): offset in seconds. Number being subtracted from 'start_time' for 'start_word' \n",
    "                        and added to 'end_time' for 'end_word'\n",
    "\n",
    "    Returns:\n",
    "        segments (array): list of tuples (start_time, end_time)\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\\tStarting the search for silence...\")\n",
    "\n",
    "    # lists for start and end times\n",
    "    starts = []\n",
    "    ends = []\n",
    "\n",
    "    for i in range(len(list_of_Words) - 1):\n",
    "        current_word = list_of_Words[i]\n",
    "        next_word = list_of_Words[i+1]\n",
    "        if next_word.start - current_word.end > threshold:\n",
    "            # find moment of silence\n",
    "            starts.append(current_word.end + offset)\n",
    "            ends.append(next_word.start - offset)\n",
    "\n",
    "    # from starts and ends to segments\n",
    "    # starts = [1, 3], ends = [2, 4] ->\n",
    "    # segments = (0, 1), (2, 3), (4, None)\n",
    "\n",
    "    segments = []\n",
    "    length = max(len(starts), len(ends))\n",
    "    for i in range(length + 1):\n",
    "        if i == 0:\n",
    "            segments.append((0, starts[0]))\n",
    "        elif i == length:\n",
    "            segments.append((ends[i-1], None))\n",
    "        else:\n",
    "            # intermediate values\n",
    "            segments.append((ends[i-1], starts[i]))\n",
    "    print(\"The search of silence is completed. Got the following array of segments: \\n\")\n",
    "    print(segments)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_video_by_segments(video, segments, result_path, bitrate=None) -> None:\n",
    "    '''\n",
    "    Crop video according to 'segments' list and\n",
    "    save final video to 'result_path'.\n",
    "\n",
    "    Parameters:\n",
    "        video: moviepy.editor.VideoFileClip object\n",
    "        segments (array): list of tuples (start_time, end_time).\n",
    "                          Received from `segments_from_audio_*()` functions\n",
    "        result_path (str): path to save final video\n",
    "        bitrate (str): bitrate for write_videofile function. \n",
    "                       Default is None, must be like '2500k', '5000k', '10000k' etc.\n",
    "    '''\n",
    "\n",
    "    print(\"\\n\\tStarting the video processing...\")\n",
    "\n",
    "    clips = []  # list of all video fragments\n",
    "    for start_seconds, end_seconds in segments:\n",
    "        # crop a video clip and add it to list\n",
    "        c = video.subclip(start_seconds, end_seconds)\n",
    "        clips.append(c)\n",
    "\n",
    "    final_clip = mp.concatenate_videoclips(clips)\n",
    "    final_clip.write_videofile(result_path, bitrate=bitrate)\n",
    "    final_clip.close()\n",
    "\n",
    "    print(\"The video processing is completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a vosk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path to vosk model downloaded from\n",
    "# https://alphacephei.com/vosk/models\n",
    "\n",
    "# vosk-model-en-us-0.21\n",
    "# vosk-model-ru-0.10\n",
    "model_path = \"models/vosk-model-en-us-0.21\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading your vosk model 'models/vosk-model-en-us-0.21'...\n",
      "'models/vosk-model-en-us-0.21' model was successfully read\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(model_path):\n",
    "    print(\"Please download the model from\" +\n",
    "          f\"https://alphacephei.com/vosk/models and unpack as {model_path}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Reading your vosk model '{model_path}'...\")\n",
    "model = Model(model_path)\n",
    "print(f\"'{model_path}' model was successfully read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to video file to convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any extensions supported by ffmpeg: \n",
    "# .ogv, .mp4, .mpeg, .avi, .mov, .mkv etc.\n",
    "video_path = \"videos/ts.mp4\"\n",
    "# new filename to save final video\n",
    "result_path = video_path[:-4] + \"_processed.mp4\"\n",
    "\n",
    "# temporary filename for audiofile (will be deleted)\n",
    "audio_path = video_path[:-3] + \"wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing method and bitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "silence = True\n",
    "# if True, process video with silence\n",
    "# if False - with control words\n",
    "\n",
    "# bitrate for write_videofile function. \n",
    "# Default is None, must be like '2500k', '5000k', '10000k' etc.\n",
    "bitrate = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next two parameters are used only if silence==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold of silence time in seconds\n",
    "threshold = 1\n",
    "# offset in seconds\n",
    "offset_silence = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next three parameters are used only if silence==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# control word that signals the beginning of the video fragment to be cut\n",
    "start_word = 'начало'\n",
    "# control word that signals the ending of the video fragment to be cut\n",
    "end_word = 'конец'\n",
    "# offset in seconds\n",
    "offset_words = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read video and convert it to mono audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if videofile exists\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"File {video_path} doesn't exist\")\n",
    "    sys.exit()\n",
    "\n",
    "# read video\n",
    "clip = mp.VideoFileClip(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  11%|█▏        | 196/1709 [00:00<00:00, 1792.48it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in videos/video_silence.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# convert video to audio\n",
    "# ffmpeg_params=[\"-ac\", \"1\"] parameter convert audio to mono format\n",
    "clip.audio.write_audiofile(audio_path, ffmpeg_params=[\"-ac\", \"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition with vosk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tStarting to convert audio to text. It may take some time...\n",
      "Done! Elapsed time = 00:00:08\n",
      "\n",
      "\tVosk thinks you said:\n",
      "\n",
      "hi everybody today we're going to talk about automatic video editing with python first of all we need to import some libraries the most important of them are of course bosque speech recognition api and movie pi \n"
     ]
    }
   ],
   "source": [
    "list_of_Words = recognize_audio_vosk(audio_path=audio_path,\n",
    "                                     model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete audio\n",
    "try:\n",
    "    os.remove(audio_path)\n",
    "except PermissionError:\n",
    "    print(f\"The file {audio_path} cannot be deleted - it is used by another process\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tStarting the search for silence...\n",
      "The search of silence is completed. Got the following array of segments: \n",
      "\n",
      "[(0, 6.61), (39.74, 44.35), (69.59, None)]\n"
     ]
    }
   ],
   "source": [
    "if silence:\n",
    "    segments = segments_from_audio_silence(list_of_Words, \n",
    "                                           threshold=threshold, \n",
    "                                           offset=offset_silence)\n",
    "else:\n",
    "    segments = segments_from_audio_control_words(list_of_Words, \n",
    "                                             start_word=start_word, \n",
    "                                             end_word=end_word,\n",
    "                                             offset=offset_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tStarting the video processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 2/422 [00:00<00:21, 19.39it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video videos/video_silence_processed.mp4.\n",
      "MoviePy - Writing audio in video_silence_processedTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 8/1148 [00:00<00:14, 79.62it/s, now=None]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/video_silence_processed.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/video_silence_processed.mp4\n",
      "The video processing is completed\n"
     ]
    }
   ],
   "source": [
    "crop_video_by_segments(video=clip,\n",
    "                       segments=segments,\n",
    "                       result_path=result_path,\n",
    "                       bitrate=bitrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
